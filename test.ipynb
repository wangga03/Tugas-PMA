{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812de706",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "--f=/run/user/1000/jupyter/runtime/kernel-v3354f02ffeeea6a339fced15295bc4273133eba82.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_path):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(csv_path)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# --------------\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# --------------\u001b[39;00m\n\u001b[1;32m     34\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: --f=/run/user/1000/jupyter/runtime/kernel-v3354f02ffeeea6a339fced15295bc4273133eba82.json"
     ]
    }
   ],
   "source": [
    "# regress_compare.py\n",
    "# ---------------------------------------------------------\n",
    "# Multivariate Linear Regression comparison:\n",
    "# 1) Normal Equation\n",
    "# 2) Batch Gradient Descent (BGD)\n",
    "# 3) Stochastic Gradient Descent (SGD)\n",
    "# 4) Mini-Batch Gradient Descent (MBGD)\n",
    "#\n",
    "# Output:\n",
    "# - 1 grafik konvergensi (PNG)\n",
    "# - 1 CSV berisi perbandingan waktu & MSE (test)\n",
    "#\n",
    "# Usage:\n",
    "#   python3 regress_compare.py /path/to/student_exam_scores.csv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import sys, os, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python3 regress_compare.py /path/to/student_exam_scores.csv\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    csv_path = sys.argv[1]\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(csv_path)\n",
    "\n",
    "    # --------------\n",
    "    # Load dataset\n",
    "    # --------------\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # --------------\n",
    "    # Target detection\n",
    "    # --------------\n",
    "    cols = list(df.columns)\n",
    "    target_candidates = [c for c in cols if c.lower() in\n",
    "                         [\"score\",\"final_score\",\"exam_score\",\"final\",\"target\",\"y\"]]\n",
    "    y_col = target_candidates[0] if target_candidates else cols[-1]\n",
    "    X_cols = [c for c in cols if c != y_col]\n",
    "\n",
    "    X = df[X_cols].select_dtypes(include=[np.number]).copy()\n",
    "    y = pd.to_numeric(df[y_col], errors=\"coerce\")\n",
    "\n",
    "    valid = ~(X.isna().any(axis=1) | y.isna())\n",
    "    X = X.loc[valid].reset_index(drop=True)\n",
    "    y = y.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    # --------------\n",
    "    # Descriptive stats (optional print)\n",
    "    # --------------\n",
    "    print(\"\\n[INFO] Features:\", list(X.columns))\n",
    "    print(\"[INFO] Target  :\", y_col)\n",
    "    print(\"\\n[Descriptive] X head:\\n\", X.head())\n",
    "    print(\"\\n[Descriptive] y head:\\n\", y.head())\n",
    "\n",
    "    # --------------\n",
    "    # Train/Test split (80/20)\n",
    "    # --------------\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = np.arange(len(X))\n",
    "    rng.shuffle(idx)\n",
    "    split = int(0.8*len(X))\n",
    "    train_idx, test_idx = idx[:split], idx[split:]\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "    y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "\n",
    "    # --------------\n",
    "    # Standardize features (fit on train)\n",
    "    # --------------\n",
    "    X_mean = X_train.mean()\n",
    "    X_std = X_train.std(ddof=0).replace(0, 1.0)\n",
    "\n",
    "    X_train_std = (X_train - X_mean) / X_std\n",
    "    X_test_std  = (X_test  - X_mean) / X_std\n",
    "\n",
    "    def add_bias(A: np.ndarray) -> np.ndarray:\n",
    "        return np.c_[np.ones((A.shape[0], 1)), A]\n",
    "\n",
    "    Xtr = add_bias(X_train_std.values)\n",
    "    Xte = add_bias(X_test_std.values)\n",
    "    ytr = y_train.values.reshape(-1, 1)\n",
    "    yte = y_test.values.reshape(-1, 1)\n",
    "\n",
    "    # Helpers\n",
    "    def mse_cost(Xm, ym, theta):\n",
    "        m = len(ym)\n",
    "        err = Xm @ theta - ym\n",
    "        return float((err.T @ err) / m)\n",
    "\n",
    "    def grad(Xm, ym, theta):\n",
    "        m = len(ym)\n",
    "        return (2/m) * (Xm.T @ (Xm @ theta - ym))\n",
    "\n",
    "    # 1) Normal Equation\n",
    "    t0 = time.perf_counter()\n",
    "    theta_ne = np.linalg.pinv(Xtr.T @ Xtr) @ Xtr.T @ ytr\n",
    "    t_ne = time.perf_counter() - t0\n",
    "    mse_ne = mse_cost(Xte, yte, theta_ne)\n",
    "\n",
    "    # 2) BGD\n",
    "    def batch_gradient_descent(Xm, ym, alpha=0.1, n_epochs=300):\n",
    "        theta = np.zeros((Xm.shape[1], 1))\n",
    "        history = []\n",
    "        for _ in range(n_epochs):\n",
    "            theta = theta - alpha * grad(Xm, ym, theta)\n",
    "            history.append(mse_cost(Xm, ym, theta))\n",
    "        return theta, history\n",
    "\n",
    "    alphas = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "    best_alpha, best_hist, best_theta_bgd = None, None, None\n",
    "    best_final_cost = math.inf\n",
    "    t0 = time.perf_counter()\n",
    "    for a in alphas:\n",
    "        theta_try, hist_try = batch_gradient_descent(Xtr, ytr, alpha=a, n_epochs=300)\n",
    "        if hist_try[-1] < best_final_cost:\n",
    "            best_final_cost = hist_try[-1]\n",
    "            best_alpha = a\n",
    "            best_hist = hist_try\n",
    "            best_theta_bgd = theta_try\n",
    "    t_bgd = time.perf_counter() - t0\n",
    "    mse_bgd = mse_cost(Xte, yte, best_theta_bgd)\n",
    "\n",
    "    # 3) SGD\n",
    "    def sgd(Xm, ym, n_epochs=50, t0_val=5.0, t1_val=50.0, shuffle=True):\n",
    "        m, n = Xm.shape\n",
    "        theta = np.zeros((n, 1))\n",
    "        history = []\n",
    "        iteration = 0\n",
    "        rng_local = np.random.default_rng(123)\n",
    "\n",
    "        def lr(t):\n",
    "            return 1.0 / (t0_val + t1_val*t)\n",
    "\n",
    "        for _ in range(n_epochs):\n",
    "            indices = np.arange(m)\n",
    "            if shuffle:\n",
    "                rng_local.shuffle(indices)\n",
    "            for i in indices:\n",
    "                xi = Xm[i:i+1].T\n",
    "                yi = ym[i:i+1]\n",
    "                grad_i = 2 * (xi @ ((xi.T @ theta) - yi))\n",
    "                eta = lr(iteration + 1)\n",
    "                theta = theta - eta * grad_i\n",
    "                iteration += 1\n",
    "            history.append(mse_cost(Xm, ym, theta))\n",
    "        return theta, history\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    theta_sgd, hist_sgd = sgd(Xtr, ytr, n_epochs=50, t0_val=5.0, t1_val=50.0, shuffle=True)\n",
    "    t_sgd = time.perf_counter() - t0\n",
    "    mse_sgd = mse_cost(Xte, yte, theta_sgd)\n",
    "\n",
    "    # 4) MBGD\n",
    "    def mbgd(Xm, ym, batch_size=16, alpha=0.05, n_epochs=150, shuffle=True):\n",
    "        m, n = Xm.shape\n",
    "        theta = np.zeros((n, 1))\n",
    "        history = []\n",
    "        rng_local = np.random.default_rng(999)\n",
    "        for _ in range(n_epochs):\n",
    "            indices = np.arange(m)\n",
    "            if shuffle:\n",
    "                rng_local.shuffle(indices)\n",
    "            for start in range(0, m, batch_size):\n",
    "                end = min(start + batch_size, m)\n",
    "                batch_idx = indices[start:end]\n",
    "                Xb = Xm[batch_idx]\n",
    "                yb = ym[batch_idx]\n",
    "                theta = theta - alpha * grad(Xb, yb, theta)\n",
    "            history.append(mse_cost(Xm, ym, theta))\n",
    "        return theta, history\n",
    "\n",
    "    batch_sizes = [8, 16, 32]\n",
    "    alphas_mbgd = [0.01, 0.05, 0.1]\n",
    "    best_cfg, best_hist_m, best_theta_m = None, None, None\n",
    "    best_final_cost_m = math.inf\n",
    "    t0 = time.perf_counter()\n",
    "    for bs in batch_sizes:\n",
    "        for a in alphas_mbgd:\n",
    "            theta_try, hist_try = mbgd(Xtr, ytr, batch_size=bs, alpha=a, n_epochs=150)\n",
    "            if hist_try[-1] < best_final_cost_m:\n",
    "                best_final_cost_m = hist_try[-1]\n",
    "                best_cfg = (bs, a)\n",
    "                best_hist_m = hist_try\n",
    "                best_theta_m = theta_try\n",
    "    t_mbgd = time.perf_counter() - t0\n",
    "    mse_mbgd = mse_cost(Xte, yte, best_theta_m)\n",
    "\n",
    "    # --------------\n",
    "    # Plot convergence (one chart)\n",
    "    # --------------\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(best_hist)+1), best_hist, label=f\"BGD (alpha={best_alpha})\")\n",
    "    plt.plot(range(1, len(hist_sgd)+1), hist_sgd, label=\"SGD\")\n",
    "    plt.plot(range(1, len(best_hist_m)+1), best_hist_m,\n",
    "             label=f\"MBGD (bs={best_cfg[0]}, alpha={best_cfg[1]})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"J(Î¸) - MSE on Train\")\n",
    "    plt.title(\"Convergence of Gradient Descent Variants\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Simpan gambar\n",
    "    plot_path = os.path.join(os.path.dirname(csv_path), \"konvergensi_gd.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, dpi=150)\n",
    "    print(f\"\\n[Saved] Plot konvergensi: {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # --------------\n",
    "    # Build comparison table\n",
    "    # --------------\n",
    "    results = pd.DataFrame([\n",
    "        {\"Metode\":\"Normal Equation\", \"Waktu Komputasi (detik)\": t_ne,   \"MSE (Test)\": mse_ne},\n",
    "        {\"Metode\":\"Batch GD\",        \"Waktu Komputasi (detik)\": t_bgd,  \"MSE (Test)\": mse_bgd},\n",
    "        {\"Metode\":\"SGD\",             \"Waktu Komputasi (detik)\": t_sgd,  \"MSE (Test)\": mse_sgd},\n",
    "        {\"Metode\":\"Mini-Batch GD\",   \"Waktu Komputasi (detik)\": t_mbgd, \"MSE (Test)\": mse_mbgd},\n",
    "    ]).sort_values(\"MSE (Test)\").reset_index(drop=True)\n",
    "\n",
    "    out_csv = os.path.join(os.path.dirname(csv_path), \"hasil_perbandingan_regresi.csv\")\n",
    "    results.to_csv(out_csv, index=False)\n",
    "    print(f\"[Saved] Tabel perbandingan: {out_csv}\\n\")\n",
    "    print(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712371a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
